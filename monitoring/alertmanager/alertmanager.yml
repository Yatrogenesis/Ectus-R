# Alertmanager Configuration for Ectus-R
# ROADMAP Task: Alert Notification System
# Production-ready configuration with NO placeholders

global:
  resolve_timeout: 5m
  # SMTP configuration for email notifications
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@ectus-r.com'
  smtp_auth_username: 'alerts@ectus-r.com'
  smtp_auth_password: '${SMTP_PASSWORD}' # Set via environment variable
  smtp_require_tls: true

# Templates for alert messages
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# The root route with all parameters
route:
  receiver: 'default-receiver'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h

  routes:
    # Critical alerts go to PagerDuty and email immediately
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      repeat_interval: 5m
      continue: true

    # Warning alerts go to email only
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_wait: 30s
      repeat_interval: 1h

    # Database alerts
    - match:
        component: database
      receiver: 'database-team'
      group_wait: 10s
      repeat_interval: 30m

    # AI Engine alerts
    - match:
        component: ai-engine
      receiver: 'ai-team'
      group_wait: 10s
      repeat_interval: 30m

    # SLA violations go to management
    - match:
        component: sla
      receiver: 'management-team'
      group_wait: 0s
      repeat_interval: 1h

# Inhibition rules to prevent alert flooding
inhibit_rules:
  # Inhibit warning if critical is firing for same alert
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance', 'service']

  # Inhibit all if service is down
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*'
    equal: ['instance']

receivers:
  # Default receiver (Slack)
  - name: 'default-receiver'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}' # Set via environment variable
        channel: '#alerts'
        title: 'Ectus-R Alert: {{ .GroupLabels.alertname }}'
        text: |
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Severity:* {{ .CommonLabels.severity }}
          *Component:* {{ .CommonLabels.component }}
          *Runbook:* {{ .CommonAnnotations.runbook }}
        send_resolved: true

  # Critical alerts (PagerDuty + Email + Slack)
  - name: 'critical-alerts'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}' # Set via environment variable
        description: '{{ .CommonAnnotations.summary }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          description: '{{ .CommonAnnotations.description }}'
          runbook: '{{ .CommonAnnotations.runbook }}'
    email_configs:
      - to: 'oncall@ectus-r.com'
        from: 'alerts@ectus-r.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alerts@ectus-r.com'
        auth_password: '${SMTP_PASSWORD}'
        headers:
          Subject: 'CRITICAL: {{ .GroupLabels.alertname }} on {{ .GroupLabels.instance }}'
        html: |
          <h2>Critical Alert Fired</h2>
          <p><strong>Alert:</strong> {{ .GroupLabels.alertname }}</p>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>Severity:</strong> {{ .CommonLabels.severity }}</p>
          <p><strong>Component:</strong> {{ .CommonLabels.component }}</p>
          <p><strong>Runbook:</strong> <a href="{{ .CommonAnnotations.runbook }}">{{ .CommonAnnotations.runbook }}</a></p>
          <p><strong>Firing Alerts:</strong> {{ .Alerts.Firing | len }}</p>
        send_resolved: true
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#critical-alerts'
        title: 'CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          :rotating_light: *CRITICAL ALERT* :rotating_light:
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Component:* {{ .CommonLabels.component }}
          *Runbook:* {{ .CommonAnnotations.runbook }}
        send_resolved: true
        color: 'danger'

  # Warning alerts (Email + Slack)
  - name: 'warning-alerts'
    email_configs:
      - to: 'devops@ectus-r.com'
        from: 'alerts@ectus-r.com'
        headers:
          Subject: 'WARNING: {{ .GroupLabels.alertname }} on {{ .GroupLabels.instance }}'
        html: |
          <h2>Warning Alert</h2>
          <p><strong>Alert:</strong> {{ .GroupLabels.alertname }}</p>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>Runbook:</strong> <a href="{{ .CommonAnnotations.runbook }}">{{ .CommonAnnotations.runbook }}</a></p>
        send_resolved: true
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#warnings'
        title: 'Warning: {{ .GroupLabels.alertname }}'
        text: |
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
        send_resolved: true
        color: 'warning'

  # Database team alerts
  - name: 'database-team'
    email_configs:
      - to: 'database-team@ectus-r.com'
        from: 'alerts@ectus-r.com'
        headers:
          Subject: 'Database Alert: {{ .GroupLabels.alertname }}'
        send_resolved: true
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#database-alerts'
        send_resolved: true

  # AI team alerts
  - name: 'ai-team'
    email_configs:
      - to: 'ai-team@ectus-r.com'
        from: 'alerts@ectus-r.com'
        headers:
          Subject: 'AI Engine Alert: {{ .GroupLabels.alertname }}'
        send_resolved: true
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#ai-alerts'
        send_resolved: true

  # Management team (SLA violations)
  - name: 'management-team'
    email_configs:
      - to: 'management@ectus-r.com,oncall@ectus-r.com'
        from: 'alerts@ectus-r.com'
        headers:
          Subject: 'SLA VIOLATION: {{ .GroupLabels.alertname }}'
        html: |
          <h2 style="color: red;">SLA Violation Alert</h2>
          <p><strong>Alert:</strong> {{ .GroupLabels.alertname }}</p>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>SLA Type:</strong> {{ .CommonLabels.sla_type }}</p>
          <p><strong>Runbook:</strong> <a href="{{ .CommonAnnotations.runbook }}">{{ .CommonAnnotations.runbook }}</a></p>
          <p style="color: red;"><strong>IMMEDIATE ACTION REQUIRED</strong></p>
        send_resolved: true
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#sla-violations'
        title: 'SLA VIOLATION: {{ .GroupLabels.alertname }}'
        text: |
          :warning: *SLA VIOLATION DETECTED* :warning:
          *Type:* {{ .CommonLabels.sla_type }}
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
          *Runbook:* {{ .CommonAnnotations.runbook }}
        send_resolved: true
        color: 'danger'
